# TT-261-RL

### Gamblerâ€™s Problem â€“ Value Iteration Solution ###

This project implements a solution to the Gamblerâ€™s Problem as described in Sutton & Barto's *Reinforcement Learning: An Introduction* (Example 4.3).

## ðŸ“Œ Problem Description

A gambler makes bets on coin flips with a goal of reaching \$100 before losing everything. On each flip:

- If heads, the gambler wins the stake.
- If tails, the gambler loses the stake.

The gambler must choose how much to bet at each step, based on current capital. The aim is to maximize the probability of reaching \$100.

The environment is modeled as an **undiscounted episodic MDP**:

- **States**: Capital values from 1 to 99
- **Actions**: Stakes from \$1 to `min(s, 100 - s)`
- **Rewards**: 1 for reaching \$100, 0 otherwise
- **Transition probability**: Depends on coin flip (with heads probability `p_h = 0.4`)

## ðŸ“ˆ Value Iteration

We apply **Value Iteration** to find:

- The **optimal state-value function** (probability of winning from each capital)
- The **final optimal policy** (stake to bet at each capital)

We iterate until the value function converges to a stable solution (change < `1e-9`).

## ðŸ§¾ Files

- `notebooks/gamblers_problem.ipynb`: Jupyter notebook with full implementation.
- `book_images/Figure_4_3.PNG`: Reference figure from the book.
- `generated_images/figure_4_3.png`: Generated value function and policy plot from this implementation.

## ðŸ“Š Output

The upper plot shows value function estimates after a few selected sweeps (1, 2, 3, and 32), along with the final value function.

The lower plot shows the final optimal policy: the amount to stake at each capital level.

<p align="center">
  <img src="./gambler-problem/generated_images/figure_4_3.png" alt="Figure 4.3 - Gambler's Problem Output" width="600">
</p>

## ðŸš€ How to Run

You can run the notebook using:

```bash
jupyter notebook notebooks/gamblers_problem.ipynb
```

### Infinite Variance â€“ Off-Policy Monte Carlo Estimation

This project implements a solution to the Infinite Variance Problem as described in Sutton & Barto's Reinforcement Learning: An Introduction (Example 5.5 and Figure 5.4).

## ðŸ“Œ Problem Description
This example demonstrates why ordinary importance sampling can suffer from infinite variance in off-policy learning.

The environment has a single non-terminal state s with two actions:

Right: Terminates immediately with reward 0.

Left:

Returns to state s with probability 0.9 (reward 0),

Terminates with probability 0.1 (reward 1).

We use two policies:

Target policy (Ï€): Always selects left â†’ true value V_Ï€(s) = 1.

Behavior policy (b): Selects left and right with equal probability (0.5 each).

The setup leads to importance sampling ratios of 2^n for n consecutive left actions, potentially resulting in very large variance.

## ðŸ“ˆ Importance Sampling

We implement ordinary importance sampling to estimate the value of state s under the target policy, using episodes generated from the behavior policy.

Each episode generates a return based on importance weights.

Due to rare but large ratios (2^n), the variance is unbounded, and estimates may not converge.

## ðŸ§¾ Files

notebooks/infinite_variance.ipynb: Jupyter notebook with full implementation.

src/infinite_variance.py: Source code for environment and policy logic.

book_images/Figure_5_4_1.PNG, Figure_5_4_2.PNG: Reference figures from the book.

generated_images/figure_5_4.png: Plot generated by this implementation.

## ðŸ“Š Output

The plot below shows 10 independent runs of the first-visit Monte Carlo algorithm using ordinary importance sampling. Even after many episodes, the estimates fluctuate and fail to reliably converge to the true value of 1.

<p align="center"> <img src="./infinite-variance/generated_images/Figure_5_4.png" alt="Figure 5.4 - Infinite Variance Problem" width="600"> </p>
ðŸš€ How to Run
You can run the notebook using:

```bash
jupyter notebook notebooks/infinite_variance.ipynb
```
