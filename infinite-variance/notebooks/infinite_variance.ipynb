{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9df227039eb6bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Infinite Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bb4d0b7d5e359",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The estimates of ordinary importance sampling will typically have infinite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have infinite variance — and this can easily happen in off-policy learning when trajectories contain loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0984d6e8d083652",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../book_images/Figure_5_4_1.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e36f254fdbaad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is only 1 non-terminal state $s$ and 2 actions, $right$ and $left$.\n",
    "- The right action causes a deterministic transition to termination.\n",
    "- The left action transitions:\n",
    "  - back to $s$ with probability 0.9,\n",
    "  - on to termination with probability 0.1.\n",
    "\n",
    "The rewards are +1 on the latter transition and otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1214161dcb0d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Consider the target policy $\\pi$ that always selects $left$, i.e. $\\pi(left | s) = 1$\n",
    "- All episodes under this policy consist of some number (possibly 0) of transitions back to $s$ followed by termination with a reward and return of +1.\n",
    "- Thus, the value of $s$ under the target policy $\\pi$ is 1 ($\\gamma$ = 1).\n",
    "\n",
    "Suppose we are estimating this value from off-policy data using the behavior policy $b$ that selects $right$ and $left$ with equal probability, i.e. $b(left | s) = 1/2 = b(right | s)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8577e7ce55c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"../book_images/Figure_5_4_2.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797df5b9cc00a4c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The lower part of Figure 5.4 shows 10 independent runs of the first-visit MC algorithm using ordinary importance sampling.\n",
    "- Even after millions of episodes, the estimates fail to converge to the correct value of 1.\n",
    "- In contrast, the weighted importance-sampling algorithm would give an estimate of exactly 1 forever after the 1st episode that ended\n",
    " with the left action.\n",
    "- All returns not equal to 1 (that is, ending with the right action) would be inconsistent with the target policy and thus would have a $\\rho_{t:T(t)-1}$ of 0 and contribute neither to the numerator nor denominator of (5.6).\n",
    "- The weighted importance sampling algorithm produces a weighted average of only the returns consistent with the target policy $\\pi$, and all of these would be exactly 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece18b7a2a5099c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.infinite_variance import actions, play\n",
    "\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd80ee35bd6e16",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of runs\n",
    "runs = 10\n",
    "\n",
    "# Number of episodes\n",
    "episodes = 100000\n",
    "\n",
    "plt.figure(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a62f2fd910243d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every run\n",
    "for run in range(runs):\n",
    "    # create an empty list for weighted rewards\n",
    "    weighted_rewards = []\n",
    "    \n",
    "    # for every episode\n",
    "    for episode in range(episodes):\n",
    "        # get reward and trajectory of actions\n",
    "        reward, trajectory = play()\n",
    "        \n",
    "        # check if the last action is 'right'\n",
    "        if trajectory[-1] == actions[\"right\"]:\n",
    "            weighted_reward = 0\n",
    "        else:\n",
    "            # calculate the importance sampling ratio\n",
    "            # The target policy always selects 'left', so π(left|s) = 1\n",
    "            # The behavior policy selects 'left' with probability 0.5, so b(left|s) = 0.5\n",
    "            # The importance ratio is π(a|s)/b(a|s) = 1/0.5 = 2 for each 'left' action\n",
    "            importance_ratio = 2 ** len(trajectory)\n",
    "            weighted_reward = importance_ratio * reward\n",
    "        \n",
    "        # append the weighted reward to the list\n",
    "        weighted_rewards.append(weighted_reward)\n",
    "    \n",
    "    # calculate the accumulative sum of weighted rewards\n",
    "    cumulative_rewards = np.cumsum(weighted_rewards)\n",
    "    \n",
    "    # calculate the estimates according to the ordinary importance sampling (Equation (5.5))\n",
    "    ordinary_estimates = cumulative_rewards / np.arange(1, episodes + 1)\n",
    "    \n",
    "    # plotting\n",
    "    plt.plot(ordinary_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbaadd150dd839",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Episodes (log scale)')\n",
    "plt.ylabel('Ordinary Importance Sampling')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c87a4cd2aae8e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.savefig('../generated_images/figure_5_4.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
